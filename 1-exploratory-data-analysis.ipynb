{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit Post Classifier: EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "english = set(nltk.corpus.words.words())\n",
    "\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 1. Getting Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We started with two tables of data from Kaggle: https://www.kaggle.com/mswarbrickjones/reddit-selfposts\n",
    "\n",
    "1. [table of 3394 subreddits, categorized]('data/subreddit_info.csv')\n",
    "2. [table of 1,033,000 text posts from 1,033 subreddits (1000 posts per subreddit)]('https://kaggle.com/mswarbrickjones/reddit-selfposts#rspct.tsv')\n",
    "\n",
    "All data is from 2016/06/01 to 2018/06/01."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### a. Getting the relevant subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "subreddits = pd.read_csv('data/subreddit_info.csv')\n",
    "subreddits.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 1. keeping only subreddits that are 'in_data'\n",
    "subreddits = subreddits[subreddits['in_data'] == True]\n",
    "\n",
    "# 2. keeping only relevant columns\n",
    "subreddits = subreddits.drop(['reason_for_exclusion', 'category_2', 'category_3', 'in_data'], axis = 1)\n",
    "\n",
    "# 3. keeping only arts and programming subreddits\n",
    "arts = subreddits[subreddits['category_1']=='arts']\n",
    "programming = subreddits[subreddits['category_1']=='programming']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### b. Getting the posts from relevant subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# this tsv is available on kaggle, linked above\n",
    "\n",
    "# # 1. reading in the tsv\n",
    "# posts = pd.read_csv('rspct.tsv', sep='\\t')\n",
    "\n",
    "# # 2. getting lists of the relevant subreddits\n",
    "# arts_subreddit_list = list(arts['subreddit'])\n",
    "# programming_subreddit_list = list(programming['subreddit'])\n",
    "\n",
    "# # 3. getting the posts into a dataframe\n",
    "# arts_list = [posts[posts['subreddit'] == subreddit] for subreddit in arts_subreddit_list]\n",
    "# arts_posts = pd.concat(arts_list)\n",
    "\n",
    "# programming_list = [posts[posts['subreddit'] == subreddit] for subreddit in programming_subreddit_list]\n",
    "# programming_posts = pd.concat(programming_list)\n",
    "\n",
    "# # 4. labelling data and dropping columns\n",
    "# arts_posts['label'] = 0\n",
    "# programming_posts['label'] = 1\n",
    "# raw = pd.concat([arts_posts, programming_posts]).reset_index().drop(['index', 'id'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 2. Cleaning/Prepping Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# # 1. combining posts' titles with their body of text\n",
    "# raw['text'] = raw['title'] + \" \" + raw['selftext']\n",
    "# posts = raw.drop(['title', 'selftext', 'subreddit'], axis=1)\n",
    "\n",
    "# # 2. dumping the cleaned data into its own csv\n",
    "# # posts.to_csv('arts-programming-reddit-posts.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 3. prepping data for NLP (tokenize, filter non-english, lemmatize, CountVectorize)\n",
    "posts = pd.read_csv('data/arts-programming-reddit-posts.csv', index_col = None).drop('Unnamed: 0', axis=1)\n",
    "\n",
    "def lemmadata(doc):\n",
    "    pattern = \"([a-zA-Z]+(?:'[a-z]+)?)\"\n",
    "    raw_tokens = nltk.regexp_tokenize(doc, pattern)\n",
    "    tokens = [i.lower() for i in raw_tokens]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    listed = [w for w in tokens if not w in stop_words]\n",
    "    lemmatized = [wordnet_lemmatizer.lemmatize(word, pos=\"v\") for word in listed]\n",
    "    lemmatized = list(filter(lambda w: w != 'lb', lemmatized))\n",
    "    words = list(filter(lambda w: w in english, lemmatized))\n",
    "    return \" \".join(words)\n",
    "\n",
    "lemmatized = pd.DataFrame([lemmadata(post) for post in list(posts['selftext'])])\n",
    "\n",
    "vec = CountVectorizer()\n",
    "X = vec.fit_transform(lemmatized[0])\n",
    "df = pd.DataFrame(X.toarray(), columns=vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = dict(df.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top100 = sorted(counts.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
